{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM63y/R/2PQbDuj5NrYKjJt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Navyasaipusala/quiz1/blob/main/q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJruRUaKYc6H"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Load dataset (example: CIFAR-10)\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.cifar10.load_data()\n",
        "x_train, x_val = x_train / 255.0, y_val / 255.0  # Normalize\n",
        "\n",
        "# Function to define a model with hyperparameters\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # First Convolutional Layer\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=hp.Int('conv1_filters', min_value=32, max_value=128, step=32),\n",
        "        kernel_size=hp.Choice('conv1_kernel', values=[3, 5]),\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.l2(1e-4),\n",
        "        input_shape=(32, 32, 3)\n",
        "    ))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "    # Second Convolutional Layer\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=hp.Int('conv2_filters', min_value=64, max_value=256, step=64),\n",
        "        kernel_size=hp.Choice('conv2_kernel', values=[3, 5]),\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.l2(1e-4)\n",
        "    ))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "    # Flatten Layer\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Fully Connected Layer\n",
        "    model.add(layers.Dense(\n",
        "        units=hp.Int('dense_units', min_value=64, max_value=512, step=64),\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.l2(1e-4)\n",
        "    ))\n",
        "    model.add(layers.Dropout(hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)))\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile Model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Use Keras Tuner for Hyperparameter Search\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    directory='tuner_results',\n",
        "    project_name='cifar10_tuning'\n",
        ")\n",
        "\n",
        "# Perform Hyperparameter Search\n",
        "tuner.search(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n",
        "\n",
        "# Get Best Model\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best Hyperparameters: {best_hps.values}\")\n",
        "\n",
        "# Train Best Model with Early Stopping\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = best_model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50,\n",
        "    validation_data=(x_val, y_val),\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate Model\n",
        "test_loss, test_acc = best_model.evaluate(x_val, y_val)\n",
        "print(f\"Validation Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Validation Loss: {test_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Step 1: Load and Preprocess the Dataset\n",
        "# - Normalization: Scaling pixel values (0 to 255) to range [0,1] for stable training.\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.cifar10.load_data()\n",
        "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
        "\n",
        "# Step 2: Define the Hyperparameter Search Model\n",
        "def build_model(hp):\n",
        "    \"\"\"\n",
        "    Model Structure with Improvements:\n",
        "    1. Convolutional Layers:\n",
        "        - Increased filter count dynamically for feature extraction.\n",
        "        - Used different kernel sizes for better spatial hierarchy learning.\n",
        "    2. Regularization:\n",
        "        - L2 Regularization to prevent overfitting.\n",
        "        - Batch Normalization to stabilize activations and speed up convergence.\n",
        "    3. Fully Connected Layers:\n",
        "        - Tuned dense units for balancing complexity and efficiency.\n",
        "        - Applied dropout for preventing co-adaptation of neurons.\n",
        "    4. Learning Rate Optimization:\n",
        "        - Experimented with different learning rates to optimize convergence.\n",
        "    \"\"\"\n",
        "\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Convolutional Layer 1: Extracts basic patterns (edges, textures)\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=hp.Int('conv1_filters', 32, 128, step=32),\n",
        "        kernel_size=hp.Choice('conv1_kernel', [3, 5]),\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.l2(1e-4),\n",
        "        input_shape=(32, 32, 3)\n",
        "    ))\n",
        "    model.add(layers.BatchNormalization())  # Stabilizes training\n",
        "    model.add(layers.MaxPooling2D(pool_size=2))  # Reduces spatial size\n",
        "\n",
        "    # Convolutional Layer 2: Extracts complex patterns\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=hp.Int('conv2_filters', 64, 256, step=64),\n",
        "        kernel_size=hp.Choice('conv2_kernel', [3, 5]),\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.l2(1e-4)\n",
        "    ))\n",
        "    model.add(layers.BatchNormalization())  # Normalization for consistency\n",
        "    model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "    # Fully Connected Layer: Decision making\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(\n",
        "        units=hp.Int('dense_units', 64, 512, step=64),\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.l2(1e-4)\n",
        "    ))\n",
        "    model.add(layers.Dropout(hp.Float('dropout', 0.2, 0.5, step=0.1)))  # Prevents overfitting\n",
        "\n",
        "    # Output Layer: Classification into 10 categories\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Optimizer: Adaptive optimization with tunable learning rate\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Step 3: Hyperparameter Optimization\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    directory='tuner_results',\n",
        "    project_name='cifar10_tuning'\n",
        ")\n",
        "\n",
        "# Search for Best Hyperparameters\n",
        "tuner.search(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n",
        "\n",
        "# Step 4: Train the Best Model with Optimized Hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Step 5: Apply Early Stopping for Efficient Training\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = best_model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50,\n",
        "    validation_data=(x_val, y_val),\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Step 6: Model Evaluation and Impact Analysis\n",
        "test_loss, test_acc = best_model.evaluate(x_val, y_val)\n",
        "print(f\"Validation Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Validation Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Logical Explanation of Impact:\n",
        "impact_description = \"\"\"\n",
        "1. Hyperparameter tuning enabled *adaptive architecture refinement*, leading to improved accuracy.\n",
        "2. *Batch Normalization* reduced internal covariate shift, stabilizing training.\n",
        "3. *L2 Regularization* prevented overfitting by controlling weight magnitudes.\n",
        "4. *Dropout* reduced over-reliance on specific neurons, improving generalization.\n",
        "5. *Learning rate tuning* optimized model convergence speed and final accuracy.\n",
        "6. *Early stopping* ensured efficiency by avoiding unnecessary epochs.\n",
        "\"\"\"\n",
        "print(impact_description)\n"
      ],
      "metadata": {
        "id": "UPAgqs9XYv-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assuming 'history' is the training history returned by model.fit()\n",
        "# Extract accuracy and loss values\n",
        "epochs = range(1, len(history.history['accuracy']) + 1)\n",
        "\n",
        "#  Visualization 1: Training & Validation Accuracy\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(epochs, history.history['accuracy'], 'bo-', label='Training Accuracy')\n",
        "plt.plot(epochs, history.history['val_accuracy'], 'r^-', label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Visualization 2: Confusion Matrix\n",
        "# Get model predictions\n",
        "y_pred = np.argmax(best_model.predict(x_val), axis=1)\n",
        "y_true = y_val.flatten()\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using Seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x7JgjE-xY0IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#  Step 1: Load the Dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "#  Step 2: Preprocess the Data\n",
        "# Normalize pixel values to range [0, 1]\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Reshape images to include channel dimension (28x28x1)\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "#  Step 3: Define the Baseline CNN Model\n",
        "def build_baseline_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')  # 10 classes\n",
        "    ])\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Step 4: Train the Model\n",
        "baseline_model = build_baseline_model()\n",
        "\n",
        "history = baseline_model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    validation_data=(x_test, y_test),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "#  Step 5: Evaluate Model Performance\n",
        "test_loss, test_acc = baseline_model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "#  Step 6: Visualize Training & Validation Accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "InJd2VpsY0j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "#  Step 1: Load and Preprocess Dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Normalize pixel values\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "#  Step 2: Define the Modified CNN Model\n",
        "def build_modified_model():\n",
        "    model = keras.Sequential([\n",
        "        # First Conv Block\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(1e-4), input_shape=(28, 28, 1)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "\n",
        "        # Second Conv Block\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "\n",
        "        # Third Conv Block\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "\n",
        "        # Flatten & Fully Connected Layers\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
        "        layers.Dropout(0.3),  # Prevents overfitting\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(10, activation='softmax')  # Output Layer for 10 classes\n",
        "    ])\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "#  Step 3: Train the Modified Model\n",
        "modified_model = build_modified_model()\n",
        "\n",
        "history = modified_model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=20,\n",
        "    validation_data=(x_test, y_test),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "#  Step 4: Evaluate Model Performance\n",
        "test_loss, test_acc = modified_model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "#  Step 5: Visualize Training & Validation Accuracy\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['accuracy'], 'bo-', label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], 'r^-', label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#  Step 6: Confusion Matrix Visualization\n",
        "y_pred = np.argmax(modified_model.predict(x_test), axis=1)\n",
        "y_true = y_test.flatten()\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PtCOa1VNZHMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#  Step 1: Evaluate on Test Set\n",
        "test_loss, test_acc = modified_model.evaluate(x_test, y_test)\n",
        "print(f\"\\n Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\" Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Step 2: Generate Predictions\n",
        "y_pred = np.argmax(modified_model.predict(x_test), axis=1)\n",
        "y_true = y_test.flatten()\n",
        "\n",
        "# Step 3: Classification Report\n",
        "print(\"\\n Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\n",
        "    \"T-shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"\n",
        "]))\n",
        "\n",
        "# Step 4: Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3cGhqt6bZLVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#  Step 1: Save the Improved Model\n",
        "model_path = \"fashion_mnist_improved_model.h5\"\n",
        "modified_model.save(model_path)\n",
        "print(f\"Model saved at: {model_path}\")\n",
        "\n",
        "# Step 2: Load the Saved Model\n",
        "loaded_model = tf.keras.models.load_model(model_path)\n",
        "print(\" Model loaded successfully!\")\n",
        "\n",
        "#  Step 3: Use the Model for Predictions\n",
        "# Select random images from the test set\n",
        "num_samples = 5\n",
        "indices = np.random.choice(len(x_test), num_samples, replace=False)\n",
        "sample_images = x_test[indices]\n",
        "sample_labels = y_test[indices]\n",
        "\n",
        "# Predict class probabilities\n",
        "predictions = loaded_model.predict(sample_images)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "#  Step 4: Display Predictions\n",
        "class_names = [\n",
        "    \"T-shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(num_samples):\n",
        "    plt.subplot(1, num_samples, i+1)\n",
        "    plt.imshow(sample_images[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(f\"Pred: {class_names[predicted_labels[i]]}\\nTrue: {class_names[sample_labels[i]]}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qK3Pj-PQZQfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#  Step 1: Generate Predictions\n",
        "y_pred = np.argmax(loaded_model.predict(x_test), axis=1)\n",
        "y_true = y_test.flatten()\n",
        "\n",
        "#  Step 2: Compute Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "#  Step 3: Plot Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7E-rS-pCZTbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#  Step 1: Extract Training History\n",
        "history_dict = history.history\n",
        "\n",
        "#  Step 2: Create Subplots\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Plot Accuracy\n",
        "ax[0].plot(history_dict['accuracy'], 'bo-', label='Training Accuracy')\n",
        "ax[0].plot(history_dict['val_accuracy'], 'r^-', label='Validation Accuracy')\n",
        "ax[0].set_title('Training vs Validation Accuracy')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Accuracy')\n",
        "ax[0].legend()\n",
        "ax[0].grid(True)\n",
        "\n",
        "# Plot Loss\n",
        "ax[1].plot(history_dict['loss'], 'bo-', label='Training Loss')\n",
        "ax[1].plot(history_dict['val_loss'], 'r^-', label='Validation Loss')\n",
        "ax[1].set_title('Training vs Validation Loss')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Loss')\n",
        "ax[1].legend()\n",
        "ax[1].grid(True)\n",
        "\n",
        "#  Step 3: Display the Plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nk3ip6YIZWRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#  Step 1: Compute Class-wise Accuracy\n",
        "y_pred = np.argmax(loaded_model.predict(x_test), axis=1)\n",
        "y_true = y_test.flatten()\n",
        "class_accuracy = [(y_pred[y_true == i] == i).mean() for i in range(10)]\n",
        "\n",
        "#  Step 2: Define Class Names\n",
        "class_names = [\n",
        "    \"T-shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"\n",
        "]\n",
        "\n",
        "#  Step 3: Plot Class-wise Accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh(class_names, class_accuracy, color='skyblue')\n",
        "plt.xlabel(\"Accuracy\")\n",
        "plt.ylabel(\"Class\")\n",
        "plt.title(\"Class-wise Accuracy\")\n",
        "plt.xlim(0.5, 1.0)  # Set x-axis between 50% and 100% for better comparison\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "import random\n",
        "\n",
        "#  Step 1: Identify Misclassified Samples\n",
        "misclassified_idx = np.where(y_pred != y_true)[0]\n",
        "misclassified_samples = random.sample(list(misclassified_idx), 9)\n",
        "\n",
        "#  Step 2: Plot Misclassified Images\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i, idx in enumerate(misclassified_samples):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')\n",
        "    plt.title(f\"Pred: {class_names[y_pred[idx]]}\\nTrue: {class_names[y_true[idx]]}\", fontsize=10)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u2DbGqf_ZZ3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "#  Load Your Own Dataset (Example: Fashion MNIST)\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "#  Normalize dataset (scaling pixel values between 0 and 1)\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "#  Define the Baseline Model\n",
        "def create_baseline_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define the Enhanced Model\n",
        "def create_enhanced_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train & Evaluate Baseline Model\n",
        "baseline_model = create_baseline_model()\n",
        "baseline_history = baseline_model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), verbose=2)\n",
        "\n",
        "# Train & Evaluate Enhanced Model\n",
        "enhanced_model = create_enhanced_model()\n",
        "enhanced_history = enhanced_model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), verbose=2)\n",
        "\n",
        "#  Compare Performance Metrics\n",
        "baseline_loss, baseline_acc = baseline_model.evaluate(x_test, y_test, verbose=0)\n",
        "enhanced_loss, enhanced_acc = enhanced_model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(\"\\n *Performance Comparison:*\")\n",
        "print(f\" Baseline Model - Accuracy: {baseline_acc*100:.2f}%, Loss: {baseline_loss:.4f}\")\n",
        "print(f\" Enhanced Model - Accuracy: {enhanced_acc*100:.2f}%, Loss: {enhanced_loss:.4f}\")\n",
        "\n",
        "# Plot Training & Validation Accuracy & Loss\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Accuracy Plot\n",
        "ax[0].plot(baseline_history.history['accuracy'], 'bo-', label='Baseline Training Acc')\n",
        "ax[0].plot(baseline_history.history['val_accuracy'], 'b--', label='Baseline Validation Acc')\n",
        "ax[0].plot(enhanced_history.history['accuracy'], 'ro-', label='Enhanced Training Acc')\n",
        "ax[0].plot(enhanced_history.history['val_accuracy'], 'r--', label='Enhanced Validation Acc')\n",
        "ax[0].set_title(\"Accuracy Comparison\")\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "ax[0].set_ylabel(\"Accuracy\")\n",
        "ax[0].legend()\n",
        "ax[0].grid(True)\n",
        "\n",
        "# Loss Plot\n",
        "ax[1].plot(baseline_history.history['loss'], 'bo-', label='Baseline Training Loss')\n",
        "ax[1].plot(baseline_history.history['val_loss'], 'b--', label='Baseline Validation Loss')\n",
        "ax[1].plot(enhanced_history.history['loss'], 'ro-', label='Enhanced Training Loss')\n",
        "ax[1].plot(enhanced_history.history['val_loss'], 'r--', label='Enhanced Validation Loss')\n",
        "ax[1].set_title(\"Loss Comparison\")\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"Loss\")\n",
        "ax[1].legend()\n",
        "ax[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix & Classification Report\n",
        "y_pred_baseline = np.argmax(baseline_model.predict(x_test), axis=1)\n",
        "y_pred_enhanced = np.argmax(enhanced_model.predict(x_test), axis=1)\n",
        "\n",
        "print(\"\\n *Baseline Model Classification Report:*\")\n",
        "print(classification_report(y_test, y_pred_baseline))\n",
        "\n",
        "print(\"\\n *Enhanced Model Classification Report:*\")\n",
        "print(classification_report(y_test, y_pred_enhanced))\n",
        "\n",
        "# Plot Confusion Matrices\n",
        "def plot_confusion_matrix(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(y_test, y_pred_baseline, \"Baseline Model Confusion Matrix\")\n",
        "plot_confusion_matrix(y_test, y_pred_enhanced, \"Enhanced Model ConfusionÂ Matrix\")\n"
      ],
      "metadata": {
        "id": "gJXfHRgPZdid"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}